Pahontu Stefan Alin 331CA

--------------------MapReduce Paradigm implemented  in Java-------------------------

        Used Java generics to create simple classes that can be inherited to
use the Map Reduce paradigm for any kind of desired input/output. These classes
are:

    Task ,which only contains the name of the file the worker will work on and
is inherited by both MapTask and ReduceTask.
        -> MapTask also contains the start and end offsets in between which a worker
        will process the document to produce partial results
        -> ReduceTask also contains the list of partial results generated by the map
        workers for a specific file

    Result<T, K>, which keeps the desired results for the MapReduce paradigm, meaning
a list of <K> and a map of <T, T> and is inherited by both MapResult and FileRes
    -> MapResult also contains the name of the processed file
    -> FileRes ( the result of a reducer worker ) also contains the name, id and rank
    of a file

    Worker<T, K> (a generic class that accepts two main types for its data structures)
       -> contains a hashmap of <T, T> (for this assignment T will represent the
       Integer class)
       -> contains a list of type <K> elements (for this assignment K will be represented
       by a String)
       -> an id and the total number of workers available
    The Worker class is inherited by both MapWorker and ReduceWorker classes
    MapWorker extends Worker<T, K>, which means it will have:
        - a map of <T, T> and a List of <K>, for this assignment (T = Integer and
        K = String)
        (PLUS)
        - map of <String, List<MapResult>> that has a list of partial results for a
        certain file
        - a list of MapTasks that need to be solved
    ReduceWorker extends Worker<T, K>, which means it will have:
        - a hashmap of <T, T> (for this assignment T will represent the
        Integer class)
        - a list of type <K> elements (for this assignment K will be represented
        as String)
        (PLUS)
        - map of <String, List<MapResult>> that has the list of all partial results
        for a certain file
        - list of strings that keeps all the file names
        - list of FileResults

    MapPool and ReducePool are just wrappers over a list of Map Workers and Reduce
    Workers respectively.

    For workers that need data types other than <Integer, String>, one would have to create
    its own MapWorker and ReduceWorker (meaning to override the run() method of the worker
    class and extend it with the needed parameters (e.g. ... extends Worker<MyType1, MyType2>))

    Flow and logic behind the program:
    - the main thread will act as the coordinator and will read the input files and
    create a list of map tasks for the first workers
    - this is done by opening a file and reading chunks of it, thus finding the start
    and end offset of a map task
    - after all the files have been read, the map workers will start
    - they will each be assigned an equal number of tasks from the list of tasks (calculated based
    on how many workers are available and how many tasks there are)
    - after the map workers finish their job, the coordinator creates the reduce tasks
    - the reduce workers then start, again being given an equal number of tasks from the
    available ones
    - after finishing the processing stage of a file, a reduce worker will calculate the
     rank of a file (since the formula is based on fibonacci stream, which has easy to find
     and widely available components, I chose to cache a global array that contains
    the first 30 numbers, for a faster implementation)
    (better than making the coordinator calculate the fibonacci array anyway XD +
    most commonly used words are definitely way shorter than 30 letters, so it should suffice)
    - after the reduce workers finish their job, the coordinator sorts the results based
    on the rank and id of a file and prints it to a file

